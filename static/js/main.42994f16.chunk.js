/*! For license information please see main.42994f16.chunk.js.LICENSE.txt */
(this["webpackJsonpproject-one"]=this["webpackJsonpproject-one"]||[]).push([[0],{35:function(e,a,t){e.exports=t.p+"static/media/teaser.03c69802.png"},47:function(e,a){e.exports="data:image/png;base64,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"},48:function(e,a,t){e.exports=t.p+"static/media/32x32.e788ac26.png"},49:function(e,a,t){e.exports=t.p+"static/media/digitLengthDist.cc510657.png"},50:function(e,a,t){e.exports=t.p+"static/media/individualDigitDist.118b3265.png"},51:function(e,a,t){e.exports=t.p+"static/media/gray.1899d1d8.png"},52:function(e,a,t){e.exports=t.p+"static/media/conv.c651a779.png"},53:function(e,a,t){e.exports=t.p+"static/media/max-pooling.8ebf998d.png"},54:function(e,a,t){e.exports=t.p+"static/media/fully-connected.3b9a9ccc.png"},55:function(e,a,t){e.exports=t.p+"static/media/oneHot.3899e148.png"},56:function(e,a,t){e.exports=t.p+"static/media/softmax.b79b7933.png"},57:function(e,a,t){e.exports=t.p+"static/media/model.d9a4b931.png"},58:function(e,a,t){e.exports=t.p+"static/media/train.56562c7b.png"},59:function(e,a,t){e.exports=t.p+"static/media/epoch.5bba88e1.png"},60:function(e,a,t){e.exports=t.p+"static/media/correct.69f6d9f3.png"},61:function(e,a,t){e.exports=t.p+"static/media/wrong.7f3224e5.png"},62:function(e,a,t){e.exports=t.p+"static/media/references.cc2d85a1.png"},71:function(e,a,t){e.exports=t(85)},81:function(e,a,t){e.exports=t.p+"static/media/bruce.c497048c.png"},82:function(e,a,t){e.exports=t.p+"static/media/tiantongli.07629d40.png"},83:function(e,a,t){e.exports=t.p+"static/media/yiqin.2892f654.png"},84:function(e,a,t){},85:function(e,a,t){"use strict";t.r(a);var n=t(0),r=t.n(n),i=t(24),o=t.n(i),l=t(16),s=t(42),c=t(43),m=t(65),d=t(64),u=t(63),h=t(111),p=t(5),g=t(112),f=t(109),b=t(106),w=t(115),x=t(108),E=t(107),v=t(105),A=t(113);function N(e){var a=e.children,t=e.window,n=Object(v.a)({disableHysteresis:!0,threshold:0,target:t?t():void 0});return r.a.cloneElement(a,{elevation:n?4:0})}var y=Object(b.a)((function(e){return{fab:{alignItems:"center"},image:{height:"auto",width:"4%"},link:{textDecoration:"none",color:"#000"},button:{textTransform:"none",color:"primary"},rightJustify:{marginLeft:"auto",marginRight:"0%"},leftJustify:{marginRight:"auto",marginLeft:"0%"}}}));function F(e){var a=y();return r.a.createElement(r.a.Fragment,null,r.a.createElement(E.a,null),r.a.createElement(N,e,r.a.createElement(w.a,{color:"secondary"},r.a.createElement(x.a,null,r.a.createElement("section",{classeName:a.leftJustify},r.a.createElement(l.b,{to:"/",className:a.link},r.a.createElement(A.a,{className:a.button},r.a.createElement(f.a,{color:"primary"},"CS4476 Project: Scene Text Detection")))),r.a.createElement("section",{className:a.rightJustify},r.a.createElement(g.a,{display:"flex",flexDirection:"row",justifyContent:"space-between"},r.a.createElement(g.a,{p:1},r.a.createElement(l.b,{to:"/proposal",className:a.link},r.a.createElement(A.a,{className:a.button},r.a.createElement(f.a,null,"Proposal")))),r.a.createElement(g.a,{p:1},r.a.createElement(l.b,{to:"/midtermUpdate",className:a.link},r.a.createElement(A.a,{className:a.button},r.a.createElement(f.a,null,"Midterm Update")))),r.a.createElement(g.a,{p:1},r.a.createElement(l.b,{to:"/finalUpdate",className:a.link},r.a.createElement(A.a,{className:a.button},r.a.createElement(f.a,null,"Final Update")))),r.a.createElement(g.a,{p:1},r.a.createElement(l.b,{to:"/projectVideo",className:a.link},r.a.createElement(A.a,{className:a.button},r.a.createElement(f.a,null,"Project Video")))),r.a.createElement(g.a,{p:1},r.a.createElement(l.b,{to:"/references",className:a.link},r.a.createElement(A.a,{className:a.button},r.a.createElement(f.a,null,"References"))))))))),r.a.createElement(x.a,null))}var T=t(27),L=t.n(T),j=t(23),k=t(110),P=Object(b.a)({grid:{marginRight:"-15px",marginLeft:"-15px",width:"auto"}});function O(e){var a=P(),t=e.children,n=e.className,i=Object(j.a)(e,["children","className"]);return r.a.createElement(k.a,Object.assign({container:!0},i,{className:a.grid+" "+n}),t)}O.defaultProps={className:""};var z=Object(b.a)({grid:{position:"relative",width:"100%",minHeight:"1px",paddingRight:"15px",paddingLeft:"15px",flexBasis:"auto"}});function B(e){var a=z(),t=e.children,n=e.className,i=Object(j.a)(e,["children","className"]);return r.a.createElement(k.a,Object.assign({item:!0},i,{className:a.grid+" "+n}),t)}B.defaultProps={className:""};var R=t(17),C={card:{border:"0",marginBottom:"30px",marginTop:"30px",borderRadius:"6px",color:"rgba(0, 0, 0, 0.87)",background:"#fff",width:"100%",boxShadow:"0 2px 2px 0 rgba(0, 0, 0, 0.14), 0 3px 1px -2px rgba(0, 0, 0, 0.2), 0 1px 5px 0 rgba(0, 0, 0, 0.12)",position:"relative",display:"flex",flexDirection:"column",minWidth:"0",wordWrap:"break-word",fontSize:".875rem",transition:"all 300ms linear"},cardPlain:{background:"transparent",boxShadow:"none"},cardCarousel:{overflow:"hidden"}},I=Object(b.a)(C);function W(e){var a,t=I(),n=e.className,i=e.children,o=e.plain,l=e.carousel,s=Object(j.a)(e,["className","children","plain","carousel"]),c=L()((a={},Object(R.a)(a,t.card,!0),Object(R.a)(a,t.cardPlain,o),Object(R.a)(a,t.cardCarousel,l),Object(R.a)(a,n,void 0!==n),a));return r.a.createElement("div",Object.assign({className:c},s),i)}var D={cardBody:{padding:"0.9375rem 1.875rem",flex:"1 1 auto"}},S=Object(b.a)(D);function H(e){var a,t=S(),n=e.className,i=e.children,o=Object(j.a)(e,["className","children"]),l=L()((a={},Object(R.a)(a,t.cardBody,!0),Object(R.a)(a,n,void 0!==n),a));return r.a.createElement("div",Object.assign({className:l},o),i)}var G=t(8),X=(Object(G.a)(Object(G.a)({},{paddingRight:"15px",paddingLeft:"15px",marginRight:"auto",marginLeft:"auto",width:"100%"}),{},{"@media (min-width: 576px)":{maxWidth:"540px"},"@media (min-width: 768px)":{maxWidth:"720px"},"@media (min-width: 992px)":{maxWidth:"960px"},"@media (min-width: 1200px)":{maxWidth:"1140px"}}),Object(G.a)({color:"#fff",background:"linear-gradient(60deg, #ffa726, #fb8c00)"},{boxShadow:"0 12px 20px -10px rgba(255, 152, 0, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(255, 152, 0, 0.2)"}),Object(G.a)({color:"#fff",background:"linear-gradient(60deg, #66bb6a, #43a047)"},{boxShadow:"0 12px 20px -10px rgba(76, 175, 80, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(76, 175, 80, 0.2)"}),Object(G.a)({color:"#fff",background:"linear-gradient(60deg, #ef5350, #e53935)"},{boxShadow:"0 12px 20px -10px rgba(244, 67, 54, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(244, 67, 54, 0.2)"}),Object(G.a)({color:"#fff",background:"linear-gradient(60deg, #26c6da, #00acc1)"},{boxShadow:"0 12px 20px -10px rgba(0, 188, 212, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(0, 188, 212, 0.2)"}),Object(G.a)({color:"#fff",background:"linear-gradient(60deg, #ab47bc, #8e24aa)"},{boxShadow:"0 12px 20px -10px rgba(156, 39, 176, 0.28), 0 4px 20px 0px rgba(0, 0, 0, 0.12), 0 7px 8px -5px rgba(156, 39, 176, 0.2)"}),Object(G.a)({color:"#fff",background:"linear-gradient(60deg, #ec407a, #d81b60)"},{boxShadow:"0 4px 20px 0px rgba(0, 0, 0, 0.14), 0 7px 10px -5px rgba(233, 30, 99, 0.4)"}),Object(G.a)({margin:"0 20px 10px",paddingTop:"10px",borderTop:"1px solid #eeeeee",height:"auto"},{fontFamily:'"Roboto", "Helvetica", "Arial", sans-serif',fontWeight:"300",lineHeight:"1.5em"}),{color:"#3C4858",margin:"1.75rem 0 0.875rem",textDecoration:"none",fontWeight:"700",fontFamily:'"Roboto Slab", "Times New Roman", serif'}),U=(Object(G.a)(Object(G.a)({},X),{},{marginTop:".625rem"}),{imgFluid:{maxWidth:"100%",height:"auto"},imgRounded:{borderRadius:"6px !important"},imgRoundedCircle:{borderRadius:"50% !important"},imgRaised:{boxShadow:"0 5px 15px -8px rgba(0, 0, 0, 0.24), 0 8px 10px -5px rgba(0, 0, 0, 0.2)"},imgGallery:{width:"100%",marginBottom:"2.142rem"},imgCardTop:{width:"100%",borderTopLeftRadius:"calc(.25rem - 1px)",borderTopRightRadius:"calc(.25rem - 1px)"},imgCardBottom:{width:"100%",borderBottomLeftRadius:"calc(.25rem - 1px)",borderBottomRightRadius:"calc(.25rem - 1px)"},imgCard:{width:"100%",borderRadius:"calc(.25rem - 1px)"},imgCardOverlay:{position:"absolute",top:"0",right:"0",bottom:"0",left:"0",padding:"1.25rem"}}),Y=Object(G.a)(Object(G.a)({section:{textAlign:"center"},title:Object(G.a)(Object(G.a)({},X),{},{marginBottom:"1rem",fontFamily:'"Space Mono"',color:"#212F3C",marginTop:"30px",minHeight:"32px",textDecoration:"none"})},U),{},{itemGrid:{marginLeft:"auto",marginRight:"auto"},cardTitle:{paddingTop:"10px",fontFamily:'"Space Mono"',color:"#212F3C"},smallTitle:{color:"#6c757d"},description:{fontFamily:"-apple-system",color:"#999"},justifyCenter:{justifyContent:"center !important"},socials:{marginTop:"0",width:"100%",transform:"none",left:"0",top:"0",height:"100%",lineHeight:"41px",fontSize:"20px",color:"#999"},margin5:{margin:"5px"}}),Q=Object(b.a)(Y);function V(){var e=Q(),a=L()(e.imgRaised,e.imgRoundedCircle,e.imgFluid);return r.a.createElement("div",{className:e.section},r.a.createElement(f.a,{variant:"h5",className:e.title},"Team Members"),r.a.createElement("div",null,r.a.createElement(O,{className:e.justifyCenter},r.a.createElement(B,{xs:8,sm:8,md:3},r.a.createElement(W,{plain:!0},r.a.createElement(B,{xs:12,sm:12,md:5,className:e.itemGrid},r.a.createElement("img",{src:t(81),alt:"...",className:a})),r.a.createElement(f.a,{className:e.cardTitle},"Bruce Qin"),r.a.createElement(H,null,r.a.createElement(f.a,{className:e.description},"I am an undergraduate Electrical Engineering student at Georgia Institute of Technology with a minor in Computer Science.",r.a.createElement("br",null),r.a.createElement("a",{href:"https://www.linkedin.com/in/bruce-qin"},"LinkedIn"))))),r.a.createElement(B,{xs:8,sm:8,md:3},r.a.createElement(W,{plain:!0},r.a.createElement(B,{xs:12,sm:12,md:5,className:e.itemGrid},r.a.createElement("img",{src:t(82),alt:"...",className:a})),r.a.createElement(f.a,{className:e.cardTitle},"Tiantong Li"),r.a.createElement(H,null,r.a.createElement(f.a,{className:e.description},"I\u2019m an undergraduate in Computer Science, focusing on Artificial Intelligence and Media. ",r.a.createElement("br",null),r.a.createElement("a",{href:"https://www.linkedin.com/in/tiantongli"},"LinkedIn"))))),r.a.createElement(B,{xs:8,sm:8,md:3},r.a.createElement(W,{plain:!0},r.a.createElement(B,{xs:12,sm:12,md:5,className:e.itemGrid},r.a.createElement("img",{src:t(83),alt:"...",className:a})),r.a.createElement(f.a,{className:e.cardTitle},"Yi Qin"),r.a.createElement(H,null,r.a.createElement(f.a,{className:e.description},"I am an undergraduate Computer Science student at Georgia Institute of Technology. Look at my profile on ",r.a.createElement("a",{href:"https://www.linkedin.com/in/yiqin1227/"},"LinkedIn."))))))))}var Z=t(47),q=t.n(Z),K=Object(b.a)((function(e){return{wrapper:{display:"flex",alignItems:"center",flexDirection:"column",paddingTop:"20px"},link:{textDecoration:"none",color:"#000"},titleFormat:{paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},titleParagraphFormat:{fontFamily:"-apple-system",paddingTop:"20px"},imageFormat:{},boxFormat:{width:"80%",paddingBottom:"30px"}}})),M=function(){var e=K();return r.a.createElement("div",{className:e.wrapper},r.a.createElement(F,null),r.a.createElement(g.a,{className:e.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h3",className:e.titleFormat},"SVHN Digit Detector"),r.a.createElement(f.a,{align:"center",variant:"subtitle1",className:e.titleParagraphFormat},"Detecting and recognizing text in natural scene images and optical character recognition is a challenging task computer scientists have been working on and improving for a long time. We want to work with the Street View House Number (SVHN) dataset to create a digit detector program. With the given input of an image taken from a camera or extracted from Google street view, our detector will extract the house numbers and display the predicted output based on the trained model.")),r.a.createElement("img",{className:e.imageFormat,src:q.a,alt:"..."}),r.a.createElement(V,null))},J=Object(b.a)((function(e){return{wrapper:{display:"flex",alignItems:"center",flexDirection:"column",paddingTop:"20px"},link:{textDecoration:"none",color:"#000"},titleFormat:{paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},lateTitleFormat:{paddingTop:"20px",paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},titleParagraphFormat:{fontFamily:"-apple-system"},titleParagraphFormat2:{fontFamily:"-apple-system",padding:"15px"},boxFormat:{width:"70%",paddingBottom:"30px"}}})),$=function(e){e.tagChange;var a=J();return r.a.createElement("div",{className:a.wrapper},r.a.createElement(F,null),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.titleFormat},"Problem Statement"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"When someone uses our system, they are expected to input images of house numbers displayed on streets. These images should be taken from a camera, or extracted from Google street view, with random background and diverse colors, but have the house numbers locating in the middle. The desired output is the same image with detected house numbers labeled besides the actual house numbers. ",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Approach"),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat},"Image Processing Techniques:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"- Convert the input image into a grayscale image ",r.a.createElement("br",null),"- Adjust lighting into desired condition in the image ",r.a.createElement("br",null),"- Denoise from the image ",r.a.createElement("br",null),"- Use the canny edge detector to detect the edges ",r.a.createElement("br",null),"- Normalising intensity of data through mean subtraction ",r.a.createElement("br",null),"- Experiment with different types of contrast normalisation (global, local) ",r.a.createElement("br",null)," ",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat}," Scene Text Detection:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"- The dataset comes in two different formats from SVHN: ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Images with character level bounding boxes ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - 32-by-32 images centered around single character ",r.a.createElement("br",null),"- For images that do not have a bounding box, use YOLO to find the text box with a single neural network to predict bounding boxes and label the bounding boxes  ",r.a.createElement("br",null),"- After the bounding boxes are determined, crop the image into individual digits. ",r.a.createElement("br",null),"- Use findCountours() function in OpenCV library to detect separate portion of the image with continuous pixels of the same color ",r.a.createElement("br",null),"- Save each number ",r.a.createElement("br",null),r.a.createElement("br",null)),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat},"Scene Text Recognition:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"- Build a convolutional neural network architecture using TensorFlow from Keras ",r.a.createElement("br",null),"- Pass in training dataset ",r.a.createElement("br",null),r.a.createElement("br",null)),r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Experiments and Results:"),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat},"Experimental Setup:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"- Download and load the testing data set ",r.a.createElement("br",null),"- Have the followings downloaded and installed: ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Python3, Numpy, OpenCV, Keras, TensorFlow ",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat},"Dataset Used:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"- ",r.a.createElement("a",{href:"http://ufldl.stanford.edu/housenumbers/"},"The Street View House Numbers (SVHN) Dataset")," ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Real-world image dataset ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Over 600,000 digit images (73,257 digits for training, 26,032 digits for testing, 531,131 additional less difficult",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 samples for extra training) ",r.a.createElement("br",null),r.a.createElement("br",null)),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat},"Implementation:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"- Retrieve an image from the testing image pool ",r.a.createElement("br",null),"- Pre-process the image using the same approach as our training approach: ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Converting to Grayscale ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Denoise ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Apply canny edge detectors ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - Find contours of the numbers ",r.a.createElement("br",null),"\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 - May need to normalise the intensity of data ",r.a.createElement("br",null),"- Use our training approach to make predictions for each letter respectively ",r.a.createElement("br",null),"- Combine the predicted letters as the answer ",r.a.createElement("br",null),"- Compare the predicted answer with the actual answer given by the testing image pool ",r.a.createElement("br",null),r.a.createElement("br",null)),r.a.createElement(f.a,{align:"left",variant:"h5",className:a.titleParagraphFormat},"Expected Results:"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"Given the images from the testing image pool as input for the above implementation, we believe that there is an intensive amount of data inside the pool, thus the dataset should be enough to test the system. We hope to measure the precision of our predicted answer compare it with the actual answer and reach a success rate of at least 80%. In order to determien the rate of success,  we will be using the number of correct detection divided by the total number of letter input. Some possible error we foresee with our mode is the effect of natural lighting conditions and other image precision factors, it is uncertain if the output for blurry/low precision images is desirable enough.")))},_=t(35),ee=t.n(_),ae=t(48),te=t.n(ae),ne=t(49),re=t.n(ne),ie=t(50),oe=t.n(ie),le=t(51),se=t.n(le),ce=t(52),me=t.n(ce),de=t(53),ue=t.n(de),he=t(54),pe=t.n(he),ge=Object(b.a)((function(e){return{wrapper:{display:"flex",alignItems:"center",flexDirection:"column",paddingTop:"20px"},link:{textDecoration:"none",color:"#000"},boxFormat:{width:"70%"},titleFormat:{paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},lateTitleFormat:{paddingTop:"20px",paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},titleParagraphFormat:{fontFamily:"-apple-system"},titleParagraphFormat2:{fontFamily:"-apple-system",padding:"10px"},imageFormat:{}}})),fe=function(e){e.tagChange;var a=ge();return r.a.createElement("div",{className:a.wrapper},r.a.createElement(F,null),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.titleFormat},"Abstract"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat},"The goal of our project is to train a convolutional neural network to detect the digits of a house number displayed on the streets. This is a basic image recognition that can be useful in many different fields. For the first update, we were able to preprocess all the images and begin setting up the different layers of our neural network.",r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:ee.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Introduction"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"As shown in the image above, our project targets images of house number displayed on the streets, we hope to achieve reasonable results given a variety of resolution of the images. Digit recognition, is becoming increasingly important in various domains as technology advances. Whether it is a mapping company needing to match images of house numbers to their geolocations, or a robot trying to locate itself through room numbers/house numbers, SVHN serves as a good basic dataset to dive into the world of image recognition. We wanted to implement our project from scratch to understand how to preprocess the dataset as well as gain a deeper understanding of how to set up and train our own neural network."),r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Approach"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"The first step was to preprocess the data from ",r.a.createElement("a",{href:"http://ufldl.stanford.edu/housenumbers/"},"The Street View House Numbers (SVHN) Dataset"),". We first focused on the first part of the dataset, where colored house-number images are given where the bounding box for each digit is given in a .mat file. The general steps we took to preprocess the image dataset was to: limit the maximum number of digit to 5 (there is one image that has 6 digits in the picture), make a new bounding box given the min and max values of the x and y values for all bounding boxes of all digits, expand the bounding box in each direction by 30% to ensure coverage of all digit details, then convert the image to grayscale. These preprocessing techniques are written with concepts we learned from class as we are first finding the bounding boxes then converting each cropped image to grayscale. After preprocessing all the images of the dataset, we began researching into how convolutional neural network are structured and how to set up each layer for our projects. Implementing and understanding the neural network has been the biggest obstacle so far as it is all of our group members' first time working with neural networks. We researched through various research papers and tutorials and have set up the network conceptually but are still in progress of coding the network.",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Experimental and Qualitative Results"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"We are working with the first section of the SVHN dataset, where we have 248,823 images with their respective digit bounding boxes and labels. As described in our approach, we began by preprocessing the .mat file similar to PS4, loading out the bounding box information as well as the labels for the digits in each image. We stored these data as a pandas dataframe for the ease of access.  The dataframe contained the filename, four extremes of the bounding boxes, labels of the digits, number of digits, the 30% increase value, as well as the original image dimensions. We then resized each individual image to 32x32 pixels as inputs to our neural network. Shown below are the outputs after cropping and resizing. ",r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:te.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"After ensuring that we have successfully cropped, resized, and labeled our images, we decided to create a set of 6000 images as the validation set that can represent our dataset. The validation set is used to evalute the given model, it will provide an unbiased evaluation of a model fit on the training dataset while tuning the hyperparameters. We take 4000 samples from training set and 2000 samples from the extra set. We plotted the digit length distribution and individual digit distribution to ensure that the validation set is a reasonable representation of our samples. We then combined all the images that are not a part of the validation set from training and extra into the final training set as the input to our neural network. The testing set is left as it is.",r.a.createElement("br",null))),r.a.createElement(f.a,null,"Digit Length Distribution"),r.a.createElement("img",{className:a.imageFormat,src:re.a,alt:"..."}),r.a.createElement(f.a,null,"Idividual Digit Distribution"),r.a.createElement("img",{className:a.imageFormat,src:oe.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"Shown below is an example of 25 images after the grayscale as well as the mean subtraction processing. These are the images that will be the input to the neural network.",r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:se.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"After the preprocessing, we will be trying to implement the neural network as the next step. First, we had to research more about neural networks as none of us have worked with one before. We decided that we will be implementing a 9 layer convolutional neural netwrok with the following architecture: ",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"center",variant:"subtitle2",className:a.titleParagraphFormat2},"INPUT -",">"," [[CONV -",">"," RELU]*2 -",">"," POOL]*2 -",">"," [[CONV -",">"," RELU]*3 -",">"," POOL] -",">"," [FC -",">"," RELU]*2 -",">"," OUTPUT",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"CONV represents a convolution layer, which uses filters to perform convolution operations with filter size F and stride S to produce the feature map. For our convolution layer, we are using a 5x5 filter as it provides a measurer for rhow close a patch of input resembles a feature, the weights in the filter matrix are derived while training the data. We will use Rectified Linear Unit (RELU) as our activation function, it aims at introducing non-linearities to the network. As our input data is of size 32x32, with 1-5 digits within one input, a small filter of size 5x5 would be more appropriate to collect representative information. The number of channels will be 1 as it should be equal to the number of color channels for the input, and our images are in gray-scale. The first 7 layers of the network are CONV-RELU layers and POOL layers. The number of filters for the first two layers are of 32, next three layers are of 64, and the next two are of 128. We increase the number as to increase the depth of the feature space, such that we can learn more levels of global abstract features. We set padding = same as the borders of the image is important and we also want to perform more convolutions without shrinking size. ",r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:me.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"POOL represents a pooling layer, which is a downsampling operation to perform spatial invariance. We will be using max-pooling layers to down-sample the input representation and reduce the input's dimensionality while keeling its max values (activated features).",r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:ue.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"FC represents a fully connected layer, which operatres on a flattened input where each input is connected to all neurons. The FC layer is at the end of the architecture and is used to optimize certain objectives. ",r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:pe.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"We are currently still in the process of setting up our neural network based on the plan above. We will be using a loss function by taking the average loss of every individual example for each of the 5 possible digits and sum them. This will be the cost function that we will be trying to minimize. We will likely be using Adam as the optimizer but futher research needs to be done. We plan to evaluate the performance of our neural network by calculating the average accuracy across all samples.",r.a.createElement("br",null)),r.a.createElement(f.a,{align:"center",variant:"h4",className:a.titleFormat},"Conclusion and Future Work"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"We are still at the most important part of the project, and the results have not been presented as our neural network is not fully set up so we have not trained our dataset. This will be the next step to our project, after training our neural network, we will be using the optimization algorithm mentioned above and then evaluate the performance for the final update. We will also continue to look into other potential preprocessing techniques other than mean-subtraction. At the midterm point, we find ourselves slightly behind where we wanted to be due to the complexity of neural network. But we believe we understand how to set up each layer and tune the hyperparameters and we will soon be training our network. More details on performance will be included in the final update.",r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null))))},be=t(55),we=t.n(be),xe=t(56),Ee=t.n(xe),ve=t(57),Ae=t.n(ve),Ne=t(58),ye=t.n(Ne),Fe=t(59),Te=t.n(Fe),Le=t(60),je=t.n(Le),ke=t(61),Pe=t.n(ke),Oe=Object(b.a)((function(e){return{wrapper:{display:"flex",alignItems:"center",flexDirection:"column",paddingTop:"20px"},link:{textDecoration:"none",color:"#000"},boxFormat:{width:"70%"},titleFormat:{paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},lateTitleFormat:{paddingTop:"20px",paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},titleParagraphFormat:{fontFamily:"-apple-system"},titleParagraphFormat2:{fontFamily:"-apple-system",padding:"10px"},imageFormat:{},imageFormat2:{paddingBottom:"30px"},imageFormat3:{paddingTop:"20px"}}})),ze=function(e){e.tagChange;var a=Oe();return r.a.createElement("div",{className:a.wrapper},r.a.createElement(F,null),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.titleFormat},"Abstract"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat},"The goal of our project is to train a convolutional neural network (CNN) to detect the digits of a house number displayed on the streets. This is a basic image recognition that can be useful in many different fields. We were able to train our neural network to perform multi-digit recognition with an accuracy rate of 94.2%.",r.a.createElement("br",null),r.a.createElement("br",null))),r.a.createElement("img",{className:a.imageFormat,src:ee.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Introduction"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"As shown in the image above, our project targets images of house numbers displayed on the streets, we hope to achieve reasonable results given a variety of resolutions of the images. Digit recognition is becoming increasingly important in various domains as technology advances. Whether it is a mapping company needing to match images of house numbers to their geolocations, or a robot trying to locate itself through room numbers/house numbers, SVHN serves as a good basic dataset to dive into the world of image recognition. We wanted to implement our project from scratch to understand how to preprocess the dataset as well as to gain a deeper understanding of how to set up and train our own neural network. Most existing ways of recognizing street view house numbers include two steps after locating where the numbers are within a picture: slicing each digit and recognizing. The ways some other projects built the neural network rely on preprocessing the picture to make boxes around each digit, and can only recognize individual digits. The output layers of their convolutional neural networks have 11 neurons, each corresponding to a possible digit (ten of which represent zero to nine, and the last one represent none). Our approach only has one step after locating the numbers in the pictures. We directly feed the portion of the image with all the numbers into the network, and train the network to predict all of the numbers at the same time."),r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Approach"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"The first step was to preprocess the data from ",r.a.createElement("a",{href:"http://ufldl.stanford.edu/housenumbers/"},"The Street View House Numbers (SVHN) Dataset"),". We first focused on the first part of the dataset, where colored house-number images are given, and the bounding box for each digit is given in a .mat file. The general steps we took to preprocess the image dataset are to: limit the maximum number of digit to 5 (there is only one image that has 6 digits in the picture), make a new bounding box given the min and max values of the x and y values for all bounding boxes of all digits, expand the bounding box in each direction by 30% to ensure the coverage of all digit details. After preprocessing all the images of the dataset, we chose to use Tensorflow.Keras to help us construct the CNN as it has many predefined methods to build the architecture. ",r.a.createElement("br",null),r.a.createElement("br",null),'As introduced in the midterm update, we were able to preprocess the images and store them in a .h5 file. The data contains cropped street view house number images of size 32x32 and their corresponding labels in the form of 5 elements lists. Each of these label lists contains 5 integers, ranging from 0 to 10, with 0 to 9 representing the actual number shown in the image, and 10 being a placeholder for images that have less than 5 numbers (If "19" appears in the image, the label list would be: [1, 9, 10, 10, 10]). To construct the CNN architecture, we began by defining the input and output layers. Our input would be the preprocessed images, which have size 32x32x3. Finding the way to construct the output layer was less intuitive for us. Common classification CNN selects and condenses important features in hidden layers, and makes the number of neurons on the output layer correspond to the number of classes the data belongs to. In this case, as each element in our labels ranges from 0 to 10, there should be 11 classes, which correspond to 11 neurons on the output layer, and the list containing the correct information for the output layer to compare with should contain 11 elements. This does not coincide with our label data set, as each of these label lists contain 5 elements. Therefore, we first further processed our labels and converted them using the one-hot encoding method. Each of these 5x1 label lists then becomes a 5x11 list, with ten 0s and one 1 located on the index corresponding to this label. Shown below is an example of how the label "19" is represented in the original label list as well as the one-hot format. ',r.a.createElement("br",null))),r.a.createElement(f.a,null,"One-Hot Encoding"),r.a.createElement("img",{className:a.imageFormat,src:we.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"With the help of this, we further designed the output layer of the network to also be in the dimension of 5x11. We individually connected 5 separate softmax densely-connected layers with 11 neurons each to their previous common fully connected layer. Then we concatenated them and reshaped them into the size of 5x11.",r.a.createElement("br",null))),r.a.createElement(f.a,null,"Design of Output Layer"),r.a.createElement("img",{className:a.imageFormat2,src:Ee.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"After settling down our input and output layers, we started to construct the hidden layers. Considering that our dataset is only recognizing numbers, which do not have very complex featured shapes and patterns, but there are 11 possible categories, we decided to use 3 convolutional layers, each is followed by 3 pooling layers, and there are 2 fully connected layers at the end. For each of these convolutional layers, we used Rectified Linear Unit(ReLU) as our activation function, as it allows an arbitrary amount of output, which corresponds to our scenario of wanting to classify into many classes. The convolutional layers have 64, 128, 256 filters, with a 7x7, 5x5, 3x3 dimension of kernel respectively. As the dimension of the kernel represents the height and width of the 2D convolution window, we decided to gradually decrease it while increasing the amount of filters to capture more essential details. As for padding, we wanted the results coming out from the first layer to preserve spatial dimensions of the volume, so we set padding to be the same for the first layer. For the second and the third, as the layers gradually identified important features, we set the padding to valid to allow the spatial dimensions to reduce via the natural application of convolution.",r.a.createElement("br",null),r.a.createElement("br",null),"Before each of these convolutional layers, we performed a batch normalization to preprocess their input, reduce the internal covariate shift, and increase the learning efficiencies. Following each convolutional layer, we used Max Pooling with a size of 2x2 to reduce the spatial dimensions of the output volume. After this, we wanted to increase the challenges for the network to learn the data by randomly dropping out some neurons on each layer, so we made each of these max pooled layers dropout with a rate of [0.3, 0.3, 0.5] respectively. For the fully connected layers. We first flattened the data and then applied two fully connected layers with sizes 1024 and 512, using them to combine the features and prepare for the final output.",r.a.createElement("br",null))),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Experimental and Qualitative Results"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"The input to the CNN is a set of 32x32x3 images, we conducted trials on black and white images first and found similar rates of success, we will be displaying the results of training and testing on colored images as this is more realistic and we do not expect users to preprocess images to black and white before using our model. Shown below is an image of the neural network model summary, showing the various layers and shapes.")),r.a.createElement(f.a,null,"Model Summary"),r.a.createElement("img",{className:a.imageFormat3,src:Ae.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"After setting up our neural network as above, we spent a lot of time tuning the various things such as the number of drop out layers, the batch size, and the number of epochs ran. We found out that having more dropout layers increases the accuracy as the act of randomly dropping neurons increases the challenge for the network to learn the data, making the network record more essential details, and thus increasing its ability for generalization. The batch size was closely related to the number of epochs, as we realized that for a smaller batch size, a smaller value for epoch should be chosen for higher accuracy. We tested batch sizes of 16 and 64, varying the epoch value between 3, 19, 15, and 20. In the end, we chose a batch size of 64 and epoch value of 10 with the lowest loss towards the end and maximum accuracy. We tested two optimizers: Adam and Root Mean Square Propagation (RMSProp). We chose RMPSprop in the end as it tries to dampen the oscillations and automatically adjust the learning rates.")),r.a.createElement(f.a,null,r.a.createElement("br",null),"Training Process"),r.a.createElement("img",{className:a.imageFormat3,src:ye.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"As shown above, the accuracy increased as the loss decreased throughout the training. We decided that 10 epochs was the right value for a batch size of 64 as we realized beyond 10 epochs the loss started to increase and the accuracy decreased. The graphs for Epoch vs. accuracy/loss are shown below.")),r.a.createElement(f.a,null,r.a.createElement("br",null),"Accuracy and Loss"),r.a.createElement("img",{className:a.imageFormat3,src:Te.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"After evaluating our model with the testing dataset, we were able to achieve a rate of 94.2%. We then plotted some images along with their respective true and predicted labels to see the performance of our trained neural network. Shown below are 25 of correct predictions and 25 of wrong predictions. The wrongly predicted images show that our network struggled to distinguish between 1 and 7 and missed some digits at times. There seem to be some errors in the original labelling, which could have affected our model.",r.a.createElement("br",null))),r.a.createElement(f.a,null,"Correct Predictions"),r.a.createElement("img",{className:a.imageFormat3,src:je.a,alt:"..."}),r.a.createElement(f.a,null,r.a.createElement("br",null),"Wrong Predictions"),r.a.createElement("img",{className:a.imageFormat3,src:Pe.a,alt:"..."}),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"These results are as expected, as the image resolution is not high, and with the images taken at different angles, it could even be hard for a human to recognize the number shown. Since we are not trying to detect the digits one by one, we don't expect our model to predict the correct number of digits for every image, as background details could be misidentified as a number potentially. Compared to a naive approach, which could perhaps make random decisions to predict the digits, our model performs much better. If the algorithm is to randomly predict the digits, the rate would be 1/(11)^N for N digits as there are a total of 11 digit possibilities (0 to 9, and None).",r.a.createElement("br",null))),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.lateTitleFormat},"Conclusion and Future Work"),r.a.createElement(f.a,{align:"left",variant:"subtitle1",className:a.titleParagraphFormat2},"Being the first CNN we have designed from scratch, we are reasonably satisfied with an accuracy of 94.2%. For future work, we believe that our model can be extended and the parameters can be tuned for better accuracy. As time was limited, we did not tune the parameters as much as we would have liked. Preprocessing could also be improved by improving the potential lighting/angle of the images. By preprocessing the images further, we could enhance some of the details of the digits and reduce the impact of background details on training. Having some problems predicting the correct number of digits was a problem as our model does not predict digit by digit but rather the entire number at once. Overall, we believe that this project has been a success.",r.a.createElement("br",null),r.a.createElement("br",null),r.a.createElement("br",null))))},Be=Object(b.a)((function(e){return{wrapper:{display:"flex",alignItems:"center",flexDirection:"column",paddingTop:"10%"},link:{textDecoration:"none",color:"#000"}}})),Re=function(e){e.tagChange;var a=Be();return r.a.createElement("div",{className:a.wrapper},r.a.createElement(F,null),r.a.createElement(f.a,null,"In progress"))},Ce=t(62),Ie=t.n(Ce),We=Object(b.a)((function(e){return{wrapper:{display:"flex",alignItems:"center",flexDirection:"column",paddingTop:"2%"},link:{textDecoration:"none",color:"#000"},titleFormat:{paddingBottom:"10px",textDecoration:"none",color:"#212F3C"},paragraphFormat:{fontFamily:"-apple-system"},boxFormat:{width:"70%"}}})),De=function(e){e.tagChange;var a=We();return r.a.createElement("div",{className:a.wrapper},r.a.createElement(F,null),r.a.createElement(g.a,{className:a.boxFormat},r.a.createElement(f.a,{align:"center",variant:"h4",className:a.titleFormat},"References")),r.a.createElement("img",{className:a.imageFormat,src:Ie.a,alt:"..."}))},Se=(t(84),Object(u.a)({palette:{primary:{main:"#000000"},secondary:{main:"#FCFFFF",contrastText:"#ffcc00"},contrastThreshold:3,tonalOffset:.2},typography:{fontFamily:['"Space Mono"'].join(",")}})),He=function(e){Object(m.a)(t,e);var a=Object(d.a)(t);function t(){return Object(s.a)(this,t),a.apply(this,arguments)}return Object(c.a)(t,[{key:"render",value:function(){return r.a.createElement("main",null,r.a.createElement(h.a,{theme:Se},r.a.createElement(p.c,null,r.a.createElement(p.a,{exact:!0,path:"/",component:M}),r.a.createElement(p.a,{exact:!0,path:"/proposal",render:function(){return r.a.createElement($,null)}}),r.a.createElement(p.a,{exact:!0,path:"/midtermUpdate",render:function(){return r.a.createElement(fe,null)}}),r.a.createElement(p.a,{exact:!0,path:"/finalUpdate",render:function(){return r.a.createElement(ze,null)}}),r.a.createElement(p.a,{exact:!0,path:"/projectVideo",render:function(){return r.a.createElement(Re,null)}}),r.a.createElement(p.a,{exact:!0,path:"/references",render:function(){return r.a.createElement(De,null)}}))))}}]),t}(r.a.Component);o.a.render(r.a.createElement(l.a,{basename:"."},r.a.createElement(He,null)),document.getElementById("root"))}},[[71,1,2]]]);
//# sourceMappingURL=main.42994f16.chunk.js.map